import gymnasium as gym
import torch
import torch.nn as nn
import numpy as np
import os
import matplotlib.pyplot as plt
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.env_util import make_vec_env

# ==========================================
# 1. ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°
# ==========================================
ENV_ID = "Acrobot-v1"
Total_Timesteps = 300_000   # ì¶©ë¶„í•œ í•™ìŠµ ì‹œê°„
Num_Envs = 8                # ë³‘ë ¬ í™˜ê²½ ê°œìˆ˜ (ì†ë„ í–¥ìƒ í•µì‹¬)
Learning_Rate = 3e-4
Save_Dir = "weights"
os.makedirs(Save_Dir, exist_ok=True)

# ==========================================
# 2. ëª¨ë¸ ë„¤íŠ¸ì›Œí¬ ì •ì˜ (Actor êµ¬ì¡°)
# ==========================================
class QNetwork(nn.Module):
    """
    SB3ì˜ MlpPolicy êµ¬ì¡°ì™€ ë™ì¼í•˜ê²Œ ë§ì¶˜ PyTorch ë„¤íŠ¸ì›Œí¬
    ë‚˜ì¤‘ì— .pt íŒŒì¼ì„ ë¡œë“œí•´ì„œ ì‚¬ìš©í•  ë•Œ í•„ìš”í•¨
    """
    def __init__(self, obs_dim, act_dim):
        super().__init__()
        # SB3 PPO ê¸°ë³¸ êµ¬ì¡°: 64x64 ë˜ëŠ” ì„¤ì •ì— ë”°ë¼ ë‹¤ë¦„.
        # ì—¬ê¸°ì„œëŠ” [128, 128]ë¡œ ë§ì¶œ ì˜ˆì •
        self.net = nn.Sequential(
            nn.Linear(obs_dim, 128), nn.Tanh(),
            nn.Linear(128, 128), nn.Tanh(),
            nn.Linear(128, act_dim)
        )

    def forward(self, x):
        return self.net(x)

def save_ppo_actor_as_pt(sb3_model, save_path, obs_dim, act_dim):
    """Stable Baselines3 ëª¨ë¸ì—ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì¶”ì¶œí•˜ì—¬ ìˆœìˆ˜ PyTorch ëª¨ë¸ë¡œ ì €ì¥"""
    custom_model = QNetwork(obs_dim, act_dim)
    
    with torch.no_grad():
        # SB3 ë‚´ë¶€ ë³€ìˆ˜ëª…ì— ë§ì¶° ê°€ì¤‘ì¹˜ ë³µì‚¬ (net_arch=[128, 128] ê¸°ì¤€)
        # Layer 1
        custom_model.net[0].weight.data = sb3_model.policy.mlp_extractor.policy_net[0].weight.data.clone()
        custom_model.net[0].bias.data = sb3_model.policy.mlp_extractor.policy_net[0].bias.data.clone()
        # Layer 2
        custom_model.net[2].weight.data = sb3_model.policy.mlp_extractor.policy_net[2].weight.data.clone()
        custom_model.net[2].bias.data = sb3_model.policy.mlp_extractor.policy_net[2].bias.data.clone()
        # Output Layer (Action Net)
        custom_model.net[4].weight.data = sb3_model.policy.action_net.weight.data.clone()
        custom_model.net[4].bias.data = sb3_model.policy.action_net.bias.data.clone()

    torch.save(custom_model.state_dict(), save_path)
    print(f"âœ… Model saved to: {save_path}")

# ==========================================
# 3. ì½œë°± (í•™ìŠµ ì¤‘ ì €ì¥ ë° ë¡œê·¸ ê¸°ë¡)
# ==========================================
class CheckpointCallback(BaseCallback):
    def __init__(self, env_id, obs_dim, act_dim, verbose=1):
        super().__init__(verbose)
        self.env_id = env_id
        self.obs_dim = obs_dim
        self.act_dim = act_dim
        self.medium_saved = False
        self.expert_saved = False
        self.reward_history = [] # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°ìš© ë°ì´í„°

    def _on_step(self) -> bool:
        # ë²¡í„° í™˜ê²½ì—ì„œëŠ” infosì— ì—í”¼ì†Œë“œ ì¢…ë£Œ ì •ë³´ê°€ ë“¤ì–´ì˜´
        for info in self.locals['infos']:
            if 'episode' in info:
                ep_rew = info['episode']['r']
                self.reward_history.append(ep_rew)
                
                # ìµœê·¼ 50ê°œ ì—í”¼ì†Œë“œ í‰ê·  ë³´ìƒ ê³„ì‚°
                if len(self.reward_history) >= 50:
                    mean_reward = np.mean(self.reward_history[-50:])
                    
                    # Medium ì €ì¥ (-200ì  ëŒíŒŒ ì‹œ)
                    if not self.medium_saved and mean_reward > -200:
                        print(f"\nğŸš€ Medium Reached! (Avg: {mean_reward:.1f})")
                        save_path = f"{Save_Dir}/{self.env_id}_medium.pt"
                        save_ppo_actor_as_pt(self.model, save_path, self.obs_dim, self.act_dim)
                        self.medium_saved = True

                    # Expert ì €ì¥ (-90ì  ëŒíŒŒ ì‹œ)
                    if not self.expert_saved and mean_reward > -90:
                        print(f"\nğŸ† Expert Reached! (Avg: {mean_reward:.1f})")
                        save_path = f"{Save_Dir}/{self.env_id}_expert.pt"
                        save_ppo_actor_as_pt(self.model, save_path, self.obs_dim, self.act_dim)
                        self.expert_saved = True
                        # Expert ë‹¬ì„± ì‹œ ì¡°ê¸° ì¢…ë£Œë¥¼ ì›í•˜ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # return False 

        return True

# ==========================================
# 4. ê²°ê³¼ ì‹œê°í™” ë° ê´€ì „ í•¨ìˆ˜
# ==========================================
def plot_learning_curve(rewards):
    """í•™ìŠµ ë³´ìƒ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°"""
    plt.figure(figsize=(10, 5))
    plt.title(f"{ENV_ID} Training Reward Curve")
    plt.plot(rewards, alpha=0.3, color='gray', label='Raw')
    
    # ì´ë™ í‰ê· 
    window = 50
    if len(rewards) >= window:
        avg = np.convolve(rewards, np.ones(window)/window, mode='valid')
        plt.plot(avg, color='red', label=f'Moving Avg ({window})')
    
    plt.axhline(-100, color='green', linestyle='--', label='Expert (-100)')
    plt.axhline(-200, color='blue', linestyle='--', label='Medium (-200)')
    plt.xlabel("Episodes")
    plt.ylabel("Reward")
    plt.legend()
    plt.grid(True, alpha=0.3)
    print("\nğŸ“Š ê·¸ë˜í”„ ì°½ì„ ë‹«ìœ¼ë©´ ì‹œë®¬ë ˆì´ì…˜ì´ ì‹œì‘ë©ë‹ˆë‹¤.")
    plt.show()

def watch_agent(model_path, obs_dim, act_dim):
    """ì €ì¥ëœ .pt ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ í™”ë©´ì— ë Œë”ë§"""
    if not os.path.exists(model_path):
        print(f"âš ï¸ {model_path} íŒŒì¼ì´ ì—†ì–´ ê´€ì „ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    print(f"\nğŸ¬ Watching Agent: {model_path}")
    env = gym.make(ENV_ID, render_mode="human")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # ëª¨ë¸ ë¡œë“œ
    model = QNetwork(obs_dim, act_dim).to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()

    # 3íŒ í”Œë ˆì´
    for ep in range(3):
        obs, _ = env.reset()
        done = False
        total_rew = 0
        while not done:
            # PPO Actor Inference
            state_t = torch.tensor(obs).float().unsqueeze(0).to(device)
            with torch.no_grad():
                logits = model(state_t)
                action = torch.argmax(logits, dim=1).item()
            
            obs, rew, terminated, truncated, _ = env.step(action)
            total_rew += rew
            done = terminated or truncated
        print(f"Episode {ep+1} Score: {total_rew:.1f}")
    
    env.close()

# ==========================================
# 5. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡
# ==========================================
if __name__ == "__main__":
    # A. í™˜ê²½ ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬)
    # n_envs=8: 8ë°° ë¹ ë¥¸ ë°ì´í„° ìˆ˜ì§‘
    env = make_vec_env(ENV_ID, n_envs=Num_Envs)
    
    obs_dim = env.observation_space.shape[0]
    act_dim = env.action_space.n
    print(f"System: {ENV_ID} | Obs: {obs_dim} | Act: {act_dim}")

    # B. PPO ëª¨ë¸ ì„¤ì •
    # net_arch=[128, 128] : QNetwork í´ë˜ìŠ¤ì™€ êµ¬ì¡°ë¥¼ ë§ì¶”ê¸° ìœ„í•¨
    policy_kwargs = dict(net_arch=dict(pi=[128, 128], vf=[128, 128]))
    
    model = PPO(
        "MlpPolicy", 
        env, 
        policy_kwargs=policy_kwargs,
        verbose=1,
        learning_rate=Learning_Rate,
        n_steps=1024,
        batch_size=64,
        ent_coef=0.0, # ë³‘ë ¬ í™˜ê²½ì´ë¼ ëœë¤ì„± ì¶©ë¶„í•¨
        tensorboard_log=f"./{ENV_ID}_tb_log/"
    )

    # C. í•™ìŠµ ì‹œì‘
    callback = CheckpointCallback(ENV_ID, obs_dim, act_dim)
    print("ğŸš€ Training Started...")
    model.learn(total_timesteps=Total_Timesteps, callback=callback)
    print("âœ… Training Finished.")

    # ë§Œì•½ Expert ì €ì¥ì´ ì•ˆ ëìœ¼ë©´ ë§ˆì§€ë§‰ ëª¨ë¸ì´ë¼ë„ ì €ì¥
    final_path = f"{Save_Dir}/{ENV_ID}_expert.pt"
    if not callback.expert_saved:
        print("âš ï¸ Expert ê¸°ì¤€ ë¯¸ë‹¬, ë§ˆì§€ë§‰ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.")
        save_ppo_actor_as_pt(model, final_path, obs_dim, act_dim)

    env.close()

    # D. ì‹œê°í™” (ê·¸ë˜í”„)
    if callback.reward_history:
        plot_learning_curve(callback.reward_history)

    # E. ê´€ì „ (Expert ëª¨ë¸)
    watch_agent(final_path, obs_dim, act_dim)